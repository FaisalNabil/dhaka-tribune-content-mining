{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZtEbFJ6aoENavAJ/kN3t4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaisalNabil/dhaka-tribune-content-mining/blob/main/NewsScrapingDhakaTribiun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "pVKTwGrCXcZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Selenium\n",
        "!pip install selenium\n",
        "\n",
        "# Set up Chromedriver\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n"
      ],
      "metadata": {
        "id": "BCfFcJJU6q75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load libraries"
      ],
      "metadata": {
        "id": "aubDDczGXfDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
        "import spacy\n",
        "import nltk\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# NLTK Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "YwSkcrrl01Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping"
      ],
      "metadata": {
        "id": "m8A3EBMkXn9h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbDUpJ9J32Fz"
      },
      "outputs": [],
      "source": [
        "# Set up headless Chrome\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.binary_location = '/usr/bin/chromium-browser'\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "class DhakaTribuneScraper:\n",
        "    def __init__(self):\n",
        "        self.driver = driver\n",
        "        self.articles = []\n",
        "\n",
        "    def scrape_year_archive(self, year):\n",
        "        start_date = date(year, 1, 1)\n",
        "        end_date = date(year, 1, 31)  # Modify end_date for the full year if needed\n",
        "        delta = timedelta(days=1)\n",
        "\n",
        "        while start_date <= end_date:\n",
        "            self.scrape_articles(start_date.strftime(\"%Y-%m-%d\"))\n",
        "            start_date += delta\n",
        "\n",
        "    def scrape_articles(self, current_date):\n",
        "        try:\n",
        "          formatted_date = current_date\n",
        "          self.base_url = f\"https://www.dhakatribune.com/archive/{formatted_date}\"\n",
        "          print(f\"Scraping page: {self.base_url}\")\n",
        "          self.driver.get(self.base_url)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping page: {self.base_url}\")\n",
        "            print(f\"General error: {e}\")\n",
        "            return\n",
        "\n",
        "        # Attempt to click the \"More\" button until it's no longer available\n",
        "        while True:\n",
        "            try:\n",
        "                more_button = WebDriverWait(self.driver, 5).until(\n",
        "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button#ajax_load_more_600_btn\"))\n",
        "                )\n",
        "                more_button.click()\n",
        "                time.sleep(2)\n",
        "            except (TimeoutException, NoSuchElementException):\n",
        "                break\n",
        "\n",
        "        # Scrape the links to individual articles\n",
        "        article_links = []\n",
        "        article_elements = self.driver.find_elements(By.CSS_SELECTOR, \"h2.title > a.link_overlay\")\n",
        "        for element in article_elements:\n",
        "            link = element.get_attribute('href')\n",
        "            if link.startswith('/'):\n",
        "                link = 'https://www.dhakatribune.com' + link\n",
        "            article_links.append(link)\n",
        "\n",
        "        # Visit each article and scrape content\n",
        "        for link in article_links:\n",
        "            try:\n",
        "                if \"photo-gallery\" in link or \"video\" in link:\n",
        "                    print(f\"Skipping non-article page: {link}\")\n",
        "                    continue\n",
        "\n",
        "                self.driver.get(link)\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.content_detail_inner\"))\n",
        "                )\n",
        "                title = self.driver.find_element(By.CSS_SELECTOR, \"h1[itemprop='headline']\").text\n",
        "                content_elements = self.driver.find_elements(By.CSS_SELECTOR, \"div.viewport > p\")\n",
        "                content = ' '.join([el.text for el in content_elements])\n",
        "                self.articles.append({'title': title, 'content': content, 'url': link, 'date': formatted_date})\n",
        "\n",
        "            except WebDriverException as e:\n",
        "                print(f\"Error occurred while scraping article: {link}\")\n",
        "                print(f\"WebDriver error: {e}\")\n",
        "                # Optionally restart the WebDriver here\n",
        "                return\n",
        "            except NoSuchElementException as e:\n",
        "                print(f\"Error occurred while scraping article: {link}\")\n",
        "                print(f\"NoSuchElement error: {e}\")\n",
        "                return\n",
        "            except TimeoutException as e:\n",
        "                print(f\"Error occurred while scraping article: {link}\")\n",
        "                print(f\"TimeoutException error: {e}\")\n",
        "                return\n",
        "\n",
        "    def close_driver(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "    def analyze_and_save(self, filename=\"dhaka_tribune_articles.csv\"):\n",
        "        # Load location data\n",
        "        with open('places-in-bangladesh.json', 'r') as file:\n",
        "            location_data = json.load(file)\n",
        "\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "        def analyze_text(text, location_data):\n",
        "            tokens = word_tokenize(text)\n",
        "            sentiment = sia.polarity_scores(text)\n",
        "            doc = nlp(text)\n",
        "            divisions, districts, subdistricts = set(), set(), set()\n",
        "\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ == \"GPE\":\n",
        "                    for division, dists in location_data.items():\n",
        "                        if ent.text in dists:\n",
        "                            divisions.add(division)\n",
        "                            districts.add(ent.text)\n",
        "                        else:\n",
        "                            for district, subs in dists.items():\n",
        "                                if ent.text in subs:\n",
        "                                    divisions.add(division)\n",
        "                                    districts.add(district)\n",
        "                                    subdistricts.add(ent.text)\n",
        "\n",
        "            return tokens, sentiment, divisions, districts, subdistricts\n",
        "\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['Date', 'Title', 'Content' 'URL', 'Sentiment', 'Divisions', 'Districts', 'Sub-Districts'])\n",
        "            for article in self.articles:\n",
        "                tokens, sentiment, divs, dists, subs = analyze_text(article['content'], location_data)\n",
        "                writer.writerow([article['date'], article['title'], article['content'], article['url'], sentiment, ', '.join(divs), ', '.join(dists), ', '.join(subs)])\n",
        "                '''print({\n",
        "                    'Title': article['title'],\n",
        "                    'URL': article['url'],\n",
        "                    'Date': article['date'],\n",
        "                    'Sentiment': sentiment,\n",
        "                    'Divisions': ', '.join(divs),\n",
        "                    'Districts': ', '.join(dists),\n",
        "                    'Sub-Districts': ', '.join(subs)\n",
        "                })'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Scrapper"
      ],
      "metadata": {
        "id": "RmPq3x7ZXtel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scraper = DhakaTribuneScraper()\n",
        "scraper.scrape_year_archive(2022)\n",
        "scraper.analyze_and_save()\n",
        "scraper.close_driver()"
      ],
      "metadata": {
        "id": "SgnwhXfkSXa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}